{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Machine Learning\n",
    "\n",
    "In this notebook, we will explore ways to analyze data, build models, and apply predictions on data corresponding to financial accounts. The idea is to grasp how machine learning works and evaluate its uses on financial systems.\n",
    "\n",
    "Here are the questions that we will try to answer during this experiment:\n",
    "\n",
    "- *What is the percentage of having a downward or upward sales for the next business year?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Initializing required libraries \n",
    "\n",
    "Here are the list of essential libraries used for building machine learning models and implementing predictions :\n",
    "\n",
    "- **Tensorflow** - *An open source software library used for conducting machine learning and deep neural networks research.*\n",
    "\n",
    "- **Numpy** - *An open source package for python used for scientific computing that supports large, multidimensional arrays and matrices, and is mainly used for data analysis.*\n",
    "\n",
    "- **Pandas** - *An open source library aimed to be the fundamental high-level building block for doing practical, real world data analysis in Python, and is mainly used for data manipulation and analysis.*\n",
    "\n",
    "- **Seaborn** - *An open source library for data visualization which provides a high-level interface for drawing attractive and informative statistical graphics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow v1.10.1\n",
      "Numpy v1.14.5\n",
      "Pandas v0.23.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "\n",
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Numpy v\" + np.__version__)\n",
    "print(\"Pandas v\" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Extraction of sample data\n",
    "\n",
    "On this part we will discuss ways on how to extract a sample of data to be analyzed. In this experiment though, we will be using an already exported CSV file of the dataset instead. For inquiries on how will the flow be when getting dataset by utilizing BigQuery, below are the sample flows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import google.datalab.bigquery as bq\n",
    "    \n",
    "# base_query = \"\"\"\n",
    "# \tselect something from data source where something = PARAMS\n",
    "# \"\"\"\n",
    "\n",
    "# query = base_query.replace(\"PARAMS\", \"params_value\")\n",
    "\n",
    "# result = bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more complex example which splits query creation into different phases of machine learning data extraction and analysis see below code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_between(start, end) :\n",
    "# \tbase_query = \"\"\"\n",
    "# \t\tselect something from source where fixed conditions are met\n",
    "# \t\"\"\"\n",
    "\n",
    "# \tconditional_sampling_a = \"and where condition respects PARAMS\"\n",
    "# \tconditional_sampling_b = \"and where condition is somewhere within {0} and {1}\".format(start, end)\n",
    "\n",
    "# \treturn \"{} \\n {} \\n {}\".format(base_query, conditional_sampling_a, conditional_sampling_b)\n",
    "\n",
    "# def create_query(phase, params_value) :\n",
    "# \t# Phases : \n",
    "# \t# \ttrain - 70% of data\n",
    "# \t# \tvalid - 15% of data\n",
    "# \t# \ttest - 15% of data\n",
    "# \tquery = \"\"\n",
    "\n",
    "# \tif phase == 'train' :\n",
    "# \t\tquery = sample_between(0, 70)\n",
    "# \telif phase == 'valid' :\n",
    "# \t\tquery sample_between(70, 85)\n",
    "# \telse :\n",
    "# \t\tquery = sample_between(85, 100)\n",
    "\n",
    "# \tquery.replace(\"PARAMS\", str(params_value))\n",
    "\t\n",
    "# \tresult = bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example consist of finding a baseline **(alpha)** for a formulated column which will be used to serve the Root-Mean-Square error **(RMSE)**,\n",
    "*a frequently used measurement of the differences between values predicted by a model or an estimator and the values observed* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import google.datalab.bigquery as bq\n",
    "\n",
    "# def compute_alpha(params_value) :\n",
    "# \tbase_query = \"\"\"\n",
    "# \t\tselect computed alpha column from source where fixed conditions are met\n",
    "# \t\"\"\"\n",
    "\n",
    "# \tconditional_sampling_a = \"and where condition respects PARAMS\"\n",
    "# \tconditional_sampling_b = \"and where condition is somewhere within all covered ranges\"\n",
    "\n",
    "# \tquery = \"{} \\n {} \\n {}\".format(base_query, conditional_sampling_a, conditional_sampling_b)\n",
    "# \tquery.replace(\"PARAMS\", str(params_value))\n",
    "\n",
    "# \tresults = bq.Query(query).execute().result().to_dataframe()\n",
    "# \talpha = results['alpha'][0]\n",
    "\n",
    "# \treturn alpha\n",
    "\n",
    "# def sample_between(start, end) :\n",
    "# \tbase_query = \"\"\"\n",
    "# \t\tselect something, and a column in need of ALPHA from source where fixed conditions are met\n",
    "# \t\"\"\"\n",
    "\n",
    "# \tconditional_sampling_a = \"and where condition respects PARAMS\"\n",
    "# \tconditional_sampling_b = \"and where condition is somewhere within {0} and {1}\".format(start, end)\n",
    "\n",
    "# \treturn \"{} \\n {} \\n {}\".format(base_query, conditional_sampling_a, conditional_sampling_b)\n",
    "\n",
    "# def compute_rmse(phase, params_value) :\n",
    "# \talpha = compute_alpha(params_value)\n",
    "# \t# Phases : \n",
    "# \t# \ttrain - 70% of data\n",
    "# \t# \tvalid - 15% of data\n",
    "# \t# \ttest - 15% of data\n",
    "# \tquery = \"\"\n",
    "\n",
    "# \tif phase == 'train' :\n",
    "# \t\tquery = sample_between(0, 70)\n",
    "# \telif phase == 'valid' :\n",
    "# \t\tquery sample_between(70, 85)\n",
    "# \telse :\n",
    "# \t\tquery = sample_between(85, 100)\n",
    "\n",
    "# \tquery.replace(\"ALPHA\", str(alpha))\n",
    "# \tquery.replace(\"PARAMS\", str(params_value))\n",
    "\n",
    "# \tresult = bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exporting dataset to csv\n",
    "\n",
    "On this part we will discuss ways to export your dataset to a csv file and also how to read from them. Here we will use the prepared csv file of sales history as source to read from.\n",
    "\n",
    "The following code shows exporting dataset to csv :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_csv(df, filename) :\n",
    "# \toutdf = df.copy(deep False)\n",
    "# \toutdf.loc[:, 'key'] = np.arrange(0, len(outdf)) # rownumber as key\n",
    "\n",
    "# \t#Re-order columns and make target column as first column\n",
    "# \tcols = outdf.columns.tolist()\n",
    "# \tcols.remove('target_column')\n",
    "# \tcols.insert(0, 'target_column')\n",
    "\n",
    "# \toutdf = outdf[cols]\n",
    "# \toutdf.to_csv(filename, header = False, index_label = False, index = False)\n",
    "\n",
    "# for phase in ['train', 'valid', 'test'] :\n",
    "# \tquery = create_query(phase, 10000)\n",
    "# \tdf = bq.Query(query).execute().result().to_dataframe()\n",
    "# \tto_csv(df, 'accounts_list-{}.csv'.format(phase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Using Pandas to train model \n",
    "\n",
    "Here we will be using Pandas way to handle datasets and train models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = ['BUSINESS_YEAR']\n",
    "TARGET = 'TOTAL_EARNINGS'\n",
    "\n",
    "columns = list([TARGET])\n",
    "columns.extend(FEATURES)\n",
    "\n",
    "df_train = pd.read_csv('./csv/sales_history_train.csv', header = None, names = columns)\n",
    "df_valid = pd.read_csv('./csv/sales_history_eval.csv', header = None, names = columns)\n",
    "\n",
    "def make_input_fn(df, num_epochs):\n",
    "\treturn tf.estimator.inputs.pandas_input_fn(\n",
    "\t\tx = df,\n",
    "\t\ty = df[TARGET],\n",
    "\t\tbatch_size = 128,\n",
    "\t\tnum_epochs = num_epochs,\n",
    "\t\tshuffle = True,\n",
    "\t\tqueue_capacity = 1000,\n",
    "\t\tnum_threads = 1\n",
    "\t)\n",
    "\n",
    "def make_prediction_input_fn(df, num_epochs):\n",
    "\treturn tf.estimator.inputs.pandas_input_fn(\n",
    "\t\tx = df,\n",
    "\t\ty = None,\n",
    "\t\tbatch_size = 128,\n",
    "\t\tnum_epochs = num_epochs,\n",
    "\t\tshuffle = True,\n",
    "\t\tqueue_capacity = 1000,\n",
    "\t\tnum_threads = 1\n",
    "\t)\n",
    "\n",
    "def make_feature_cols():\n",
    "\tinput_columns = [tf.feature_column.numeric_column(k) for k in FEATURES]\n",
    "\treturn input_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Linear Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './pandas/linear/sales_history_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc32cd29978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./pandas/linear/sales_history_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.068113e+20, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 2 into ./pandas/linear/sales_history_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.9212257e+18.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-14-08:56:20\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./pandas/linear/sales_history_trained/model.ckpt-2\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-14-08:56:20\n",
      "INFO:tensorflow:Saving dict for global step 2: average_loss = 1.6947793e+16, global_step = 2, label/mean = 75158264.0, loss = 1.8642572e+17, prediction/mean = 438.9549\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2: ./pandas/linear/sales_history_trained/model.ckpt-2\n",
      "RMSE on validation dataset = 130183688.0\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./pandas/linear/sales_history_trained/model.ckpt-2\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "{'predictions': array([438.9549], dtype=float32)}\n",
      "{'predictions': array([438.519], dtype=float32)}\n",
      "{'predictions': array([437.86514], dtype=float32)}\n",
      "{'predictions': array([437.86514], dtype=float32)}\n",
      "{'predictions': array([438.0831], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# Using Linear Regression\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "OUTDIR = './pandas/linear/sales_history_trained'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "model = tf.estimator.LinearRegressor(\n",
    "\tfeature_columns = make_feature_cols(), model_dir = OUTDIR)\n",
    "\n",
    "model.train(input_fn = make_input_fn(df_train, num_epochs = 10))\n",
    "\n",
    "def print_rmse(model, name, df):\n",
    "\tmetrics = model.evaluate(input_fn = make_input_fn(df, 1))\n",
    "\tprint('RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))\n",
    "\n",
    "print_rmse(model, 'validation', df_valid)\n",
    "\n",
    "predictions = model.predict(input_fn = make_prediction_input_fn(df_valid, 5))\n",
    "for i in range(5):\n",
    "\tprint(next(predictions))\n",
    "\n",
    "# End of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Deep Neural Network Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './pandas/dnn/sales_history_trained', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc32cd29780>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./pandas/dnn/sales_history_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.88967e+19, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 13 into ./pandas/dnn/sales_history_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.077775e+19.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-09-14-08:56:22\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./pandas/dnn/sales_history_trained/model.ckpt-13\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-09-14-08:56:22\n",
      "INFO:tensorflow:Saving dict for global step 13: average_loss = 1.6947859e+16, global_step = 13, label/mean = 75158264.0, loss = 1.8642646e+17, prediction/mean = 0.29932672\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 13: ./pandas/dnn/sales_history_trained/model.ckpt-13\n",
      "RMSE on validation dataset = 130183944.0\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./pandas/dnn/sales_history_trained/model.ckpt-13\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "{'predictions': array([0.29932672], dtype=float32)}\n",
      "{'predictions': array([0.29932672], dtype=float32)}\n",
      "{'predictions': array([0.29932672], dtype=float32)}\n",
      "{'predictions': array([0.29932672], dtype=float32)}\n",
      "{'predictions': array([0.29932672], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# Using DNN\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "OUTDIR = './pandas/dnn/sales_history_trained'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True)\n",
    "\n",
    "model = tf.estimator.DNNRegressor(hidden_units = [32, 8, 2],\n",
    "\tfeature_columns = make_feature_cols(), model_dir = OUTDIR)\n",
    "\n",
    "model.train(input_fn = make_input_fn(df_train, num_epochs = 100));\n",
    "\n",
    "print_rmse(model, 'validation', df_valid)\n",
    "\n",
    "predictions = model.predict(input_fn = make_prediction_input_fn(df_valid, 5))\n",
    "for i in range(5):\n",
    "\tprint(next(predictions))\n",
    "\n",
    "# End of DNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
