{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Machine Learning\n",
    "\n",
    "In this notebook, we will explore ways to analyze data, build models, and apply predictions on data corresponding to financial accounts. The idea is to grasp how machine learning works and evaluate its uses on financial systems.\n",
    "\n",
    "Here are the questions that we will try to answer during this experiment:\n",
    "\n",
    "- *What is the percentage of having a downward or upward sales for the next business year?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initializing required libraries \n",
    "\n",
    "Here are the list of essential libraries used for building machine learning models and implementing predictions :\n",
    "\n",
    "- **Tensorflow** - *An open source software library used for conducting machine learning and deep neural networks research.*\n",
    "\n",
    "- **Numpy** - *An open source package for python used for scientific computing that supports large, multidimensional arrays and matrices, and is mainly used for data analysis.*\n",
    "\n",
    "- **Pandas** - *An open source library aimed to be the fundamental high-level building block for doing practical, real world data analysis in Python, and is mainly used for data manipulation and analysis.*\n",
    "\n",
    "- **Seaborn** - *An open source library for data visualization which provides a high-level interface for drawing attractive and informative statistical graphics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow v1.10.1\n",
      "Numpy v1.14.5\n",
      "Pandas v0.23.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "\n",
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Numpy v\" + np.__version__)\n",
    "print(\"Pandas v\" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extraction of sample data\n",
    "\n",
    "On this part we will discuss ways on how to extract a sample of data to be analyzed. In this experiment though, we will be using an already exported CSV file of the dataset instead. For inquiries on how will the flow be when getting dataset by utilizing BigQuery, below are the sample flows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import google.datalab.bigquery as bq\n",
    "    \n",
    "# base_query = \"\"\"\n",
    "#     select something from data source where something = PARAMS\n",
    "# \"\"\"\n",
    "\n",
    "# query = base_query.replace(\"PARAMS\", \"params_value\")\n",
    "\n",
    "# result = bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more complex example which splits query creation into different phases of machine learning data extraction and analysis see below code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_between(start, end) :\n",
    "# \tbase_query = \"\"\"\n",
    "# \t\tselect something from source where fixed conditions are met\n",
    "# \t\"\"\"\n",
    "\n",
    "# \tconditional_sampling_a = \"and where condition respects PARAMS\"\n",
    "# \tconditional_sampling_b = \"and where condition is somewhere within {0} and {1}\".format(start, end)\n",
    "\n",
    "# \treturn \"{} \\n {} \\n {}\".format(base_query, conditional_sampling_a, conditional_sampling_b)\n",
    "\n",
    "# def create_query(phase, params_value) :\n",
    "# \t# Phases : \n",
    "# \t# \ttrain - 70% of data\n",
    "# \t# \tvalid - 15% of data\n",
    "# \t# \ttest - 15% of data\n",
    "# \tquery = \"\"\n",
    "\n",
    "# \tif phase == 'train' :\n",
    "# \t\tquery = sample_between(0, 70)\n",
    "# \telif phase == 'valid' :\n",
    "# \t\tquery sample_between(70, 85)\n",
    "# \telse :\n",
    "# \t\tquery = sample_between(85, 100)\n",
    "\n",
    "# \tquery.replace(\"PARAMS\", str(params_value))\n",
    "\t\n",
    "# \tresult = bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example consist of finding a baseline **(alpha)** for a formulated column which will be used to serve the Root-Mean-Square error **(RMSE)**,\n",
    "*a frequently used measurement of the differences between values predicted by a model or an estimator and the values observed* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import google.datalab.bigquery as bq\n",
    "\n",
    "# def compute_alpha(params_value) :\n",
    "# \tbase_query = \"\"\"\n",
    "# \t\tselect computed alpha column from source where fixed conditions are met\n",
    "# \t\"\"\"\n",
    "\n",
    "# \tconditional_sampling_a = \"and where condition respects PARAMS\"\n",
    "# \tconditional_sampling_b = \"and where condition is somewhere within all covered ranges\"\n",
    "\n",
    "# \tquery = \"{} \\n {} \\n {}\".format(base_query, conditional_sampling_a, conditional_sampling_b)\n",
    "# \tquery.replace(\"PARAMS\", str(params_value))\n",
    "\n",
    "# \tresults = bq.Query(query).execute().result().to_dataframe()\n",
    "# \talpha = results['alpha'][0]\n",
    "\n",
    "# \treturn alpha\n",
    "\n",
    "# def sample_between(start, end) :\n",
    "# \tbase_query = \"\"\"\n",
    "# \t\tselect something, and a column in need of ALPHA from source where fixed conditions are met\n",
    "# \t\"\"\"\n",
    "\n",
    "# \tconditional_sampling_a = \"and where condition respects PARAMS\"\n",
    "# \tconditional_sampling_b = \"and where condition is somewhere within {0} and {1}\".format(start, end)\n",
    "\n",
    "# \treturn \"{} \\n {} \\n {}\".format(base_query, conditional_sampling_a, conditional_sampling_b)\n",
    "\n",
    "# def compute_rmse(phase, params_value) :\n",
    "# \talpha = compute_alpha(params_value)\n",
    "# \t# Phases : \n",
    "# \t# \ttrain - 70% of data\n",
    "# \t# \tvalid - 15% of data\n",
    "# \t# \ttest - 15% of data\n",
    "# \tquery = \"\"\n",
    "\n",
    "# \tif phase == 'train' :\n",
    "# \t\tquery = sample_between(0, 70)\n",
    "# \telif phase == 'valid' :\n",
    "# \t\tquery sample_between(70, 85)\n",
    "# \telse :\n",
    "# \t\tquery = sample_between(85, 100)\n",
    "\n",
    "# \tquery.replace(\"ALPHA\", str(alpha))\n",
    "# \tquery.replace(\"PARAMS\", str(params_value))\n",
    "\n",
    "# \tresult = bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reading and exporting dataset to csv\n",
    "\n",
    "On this part we will discuss ways to export your dataset to a csv file and also how to read from them. Here we will use the prepared csv file of sales history as source to read from.\n",
    "\n",
    "The following code shows exporting dataset to csv :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_csv(df, filename) :\n",
    "# \toutdf = df.copy(deep False)\n",
    "# \toutdf.loc[:, 'key'] = np.arrange(0, len(outdf)) # rownumber as key\n",
    "\n",
    "# \t#Re-order columns and make target column as first column\n",
    "# \tcols = outdf.columns.tolist()\n",
    "# \tcols.remove('target_column')\n",
    "# \tcols.insert(0, 'target_column')\n",
    "\n",
    "# \toutdf = outdf[cols]\n",
    "# \toutdf.to_csv(filename, header = False, index_label = False, index = False)\n",
    "\n",
    "# for phase in ['train', 'valid', 'test'] :\n",
    "# \tquery = create_query(phase, 10000)\n",
    "# \tdf = bq.Query(query).execute().result().to_dataframe()\n",
    "# \tto_csv(df, 'accounts_list-{}.csv'.format(phase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code for reading and decoding from a csv file, *note that if you still have unnecessary columns in your csv file please only declare those columns that are features and leave out the rest* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = ['BUSINESS_YEAR']\n",
    "# TARGET = 'TOTAL_EARNINGS'\n",
    "\n",
    "# columns = list([TARGET])\n",
    "# columns.extend(FEATURES)\n",
    "\n",
    "# df_train = pd.read_csv('./csv/sales_history_train.csv', header = None, names = columns)\n",
    "# df_valid = pd.read_csv('./csv/sales_history_eval.csv', header = None, names = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more complex way of reading thru csv file, we will use an input function that reads in batches instead of using Pandas :"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
