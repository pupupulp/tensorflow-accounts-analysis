{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Machine Learning\n",
    "\n",
    "In this notebook, we will explore ways to analyze data, build models, and apply predictions on data corresponding to financial accounts. The idea is to grasp how machine learning works and evaluate its uses on financial systems.\n",
    "\n",
    "Here are the questions that we will try to answer during this experiment:\n",
    "\n",
    "- *What is the percentage of having a downward or upward sales for the next business year?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initializing required libraries \n",
    "\n",
    "Here are the list of essential libraries used for building machine learning models and implementing predictions :\n",
    "\n",
    "- **Tensorflow** - *An open source software library used for conducting machine learning and deep neural networks research.*\n",
    "\n",
    "- **Numpy** - *An open source package for python used for scientific computing that supports large, multidimensional arrays and matrices, and is mainly used for data analysis.*\n",
    "\n",
    "- **Pandas** - *An open source library aimed to be the fundamental high-level building block for doing practical, real world data analysis in Python, and is mainly used for data manipulation and analysis.*\n",
    "\n",
    "- **Seaborn** - *An open source library for data visualization which provides a high-level interface for drawing attractive and informative statistical graphics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow v1.10.1\n",
      "Numpy v1.14.5\n",
      "Pandas v0.23.4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "\n",
    "print(\"TensorFlow v\" + tf.__version__)\n",
    "print(\"Numpy v\" + np.__version__)\n",
    "print(\"Pandas v\" + pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extraction of sample data\n",
    "\n",
    "On this part we will discuss ways on how to extract a sample of data to be analyzed. In this experiment though, we will be using an already exported CSV file of the dataset instead. For inquiries on how will the flow be when getting dataset by utilizing BigQuery, below are the sample flows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import google.datalab.bigquery as bq\n",
    "    \n",
    "# base_query = \"\"\"\n",
    "# \tselect something from data source where something = PARAMS\n",
    "# \"\"\"\n",
    "\n",
    "# query = base_query.replace(\"PARAMS\", \"params_value\")\n",
    "\n",
    "# result = bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more complex example which splits query creation into different phases of machine learning data extraction and analysis see below code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_between(start, end) :\n",
    "# \tbase_query = \"\"\"\n",
    "# \t\tselect something from source where fixed conditions are met\n",
    "# \t\"\"\"\n",
    "\n",
    "# \tconditional_sampling_a = \"and where condition respects PARAMS\"\n",
    "# \tconditional_sampling_b = \"and where condition is somewhere within {0} and {1}\".format(start, end)\n",
    "\n",
    "# \treturn \"{} \\n {} \\n {}\".format(base_query, conditional_sampling_a, conditional_sampling_b)\n",
    "\n",
    "# def create_query(phase, params_value) :\n",
    "# \t# Phases : \n",
    "# \t# \ttrain - 70% of data\n",
    "# \t# \tvalid - 15% of data\n",
    "# \t# \ttest - 15% of data\n",
    "# \tquery = \"\"\n",
    "\n",
    "# \tif phase == 'train' :\n",
    "# \t\tquery = sample_between(0, 70)\n",
    "# \telif phase == 'valid' :\n",
    "# \t\tquery sample_between(70, 85)\n",
    "# \telse :\n",
    "# \t\tquery = sample_between(85, 100)\n",
    "\n",
    "# \tquery.replace(\"PARAMS\", str(params_value))\n",
    "\t\n",
    "# \tresult = bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example consist of finding a baseline **(alpha)** for a formulated column which will be used to serve the Root-Mean-Square error **(RMSE)**,\n",
    "*a frequently used measurement of the differences between values predicted by a model or an estimator and the values observed* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import google.datalab.bigquery as bq\n",
    "\n",
    "# def compute_alpha(params_value) :\n",
    "# \tbase_query = \"\"\"\n",
    "# \t\tselect computed alpha column from source where fixed conditions are met\n",
    "# \t\"\"\"\n",
    "\n",
    "# \tconditional_sampling_a = \"and where condition respects PARAMS\"\n",
    "# \tconditional_sampling_b = \"and where condition is somewhere within all covered ranges\"\n",
    "\n",
    "# \tquery = \"{} \\n {} \\n {}\".format(base_query, conditional_sampling_a, conditional_sampling_b)\n",
    "# \tquery.replace(\"PARAMS\", str(params_value))\n",
    "\n",
    "# \tresults = bq.Query(query).execute().result().to_dataframe()\n",
    "# \talpha = results['alpha'][0]\n",
    "\n",
    "# \treturn alpha\n",
    "\n",
    "# def sample_between(start, end) :\n",
    "# \tbase_query = \"\"\"\n",
    "# \t\tselect something, and a column in need of ALPHA from source where fixed conditions are met\n",
    "# \t\"\"\"\n",
    "\n",
    "# \tconditional_sampling_a = \"and where condition respects PARAMS\"\n",
    "# \tconditional_sampling_b = \"and where condition is somewhere within {0} and {1}\".format(start, end)\n",
    "\n",
    "# \treturn \"{} \\n {} \\n {}\".format(base_query, conditional_sampling_a, conditional_sampling_b)\n",
    "\n",
    "# def compute_rmse(phase, params_value) :\n",
    "# \talpha = compute_alpha(params_value)\n",
    "# \t# Phases : \n",
    "# \t# \ttrain - 70% of data\n",
    "# \t# \tvalid - 15% of data\n",
    "# \t# \ttest - 15% of data\n",
    "# \tquery = \"\"\n",
    "\n",
    "# \tif phase == 'train' :\n",
    "# \t\tquery = sample_between(0, 70)\n",
    "# \telif phase == 'valid' :\n",
    "# \t\tquery sample_between(70, 85)\n",
    "# \telse :\n",
    "# \t\tquery = sample_between(85, 100)\n",
    "\n",
    "# \tquery.replace(\"ALPHA\", str(alpha))\n",
    "# \tquery.replace(\"PARAMS\", str(params_value))\n",
    "\n",
    "# \tresult = bq.Query(query).execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exporting dataset to csv\n",
    "\n",
    "On this part we will discuss ways to export your dataset to a csv file and also how to read from them. Here we will use the prepared csv file of sales history as source to read from.\n",
    "\n",
    "The following code shows exporting dataset to csv :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_csv(df, filename) :\n",
    "# \toutdf = df.copy(deep False)\n",
    "# \toutdf.loc[:, 'key'] = np.arrange(0, len(outdf)) # rownumber as key\n",
    "\n",
    "# \t#Re-order columns and make target column as first column\n",
    "# \tcols = outdf.columns.tolist()\n",
    "# \tcols.remove('target_column')\n",
    "# \tcols.insert(0, 'target_column')\n",
    "\n",
    "# \toutdf = outdf[cols]\n",
    "# \toutdf.to_csv(filename, header = False, index_label = False, index = False)\n",
    "\n",
    "# for phase in ['train', 'valid', 'test'] :\n",
    "# \tquery = create_query(phase, 10000)\n",
    "# \tdf = bq.Query(query).execute().result().to_dataframe()\n",
    "# \tto_csv(df, 'accounts_list-{}.csv'.format(phase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using Pandas to read datasets and train model\n",
    "\n",
    "Here we will be using Pandas way to handle datasets and train models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = ['BUSINESS_YEAR']\n",
    "# TARGET = 'TOTAL_EARNINGS'\n",
    "\n",
    "# columns = list([TARGET])\n",
    "# columns.extend(FEATURES)\n",
    "\n",
    "# df_train = pd.read_csv('./csv/sales_history_train.csv', header = None, names = columns)\n",
    "# df_valid = pd.read_csv('./csv/sales_history_eval.csv', header = None, names = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting training and validation datasets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_train = sns.regplot(x = \"TOTAL_EARNINGS\", y = \"BUSINESS_YEAR\", ci = None, truncate = True, data = df_train)\n",
    "# plot_valid = sns.regplot(x = \"TOTAL_EARNINGS\", y = \"BUSINESS_YEAR\", ci = None, truncate = True, data = df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defination of input function for both trainer and prediction, and make feature function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_input_fn(df, num_epochs):\n",
    "# \treturn tf.estimator.inputs.pandas_input_fn(\n",
    "# \t\tx = df,\n",
    "# \t\ty = df[TARGET],\n",
    "# \t\tbatch_size = 128,\n",
    "# \t\tnum_epochs = num_epochs,\n",
    "# \t\tshuffle = True,\n",
    "# \t\tqueue_capacity = 1000,\n",
    "# \t\tnum_threads = 1\n",
    "# \t)\n",
    "\n",
    "# def make_prediction_input_fn(df, num_epochs):\n",
    "# \treturn tf.estimator.inputs.pandas_input_fn(\n",
    "# \t\tx = df,\n",
    "# \t\ty = None,\n",
    "# \t\tbatch_size = 128,\n",
    "# \t\tnum_epochs = num_epochs,\n",
    "# \t\tshuffle = True,\n",
    "# \t\tqueue_capacity = 1000,\n",
    "# \t\tnum_threads = 1\n",
    "# \t)\n",
    "\n",
    "# def make_feature_cols():\n",
    "# \tinput_columns = [tf.feature_column.numeric_column(k) for k in FEATURES]\n",
    "# \treturn input_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation using Linear Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Linear Regression\n",
    "\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# OUTDIR = './pandas/linear/sales_history_trained'\n",
    "# shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "\n",
    "# model = tf.estimator.LinearRegressor(\n",
    "# \tfeature_columns = make_feature_cols(), model_dir = OUTDIR)\n",
    "\n",
    "# model.train(input_fn = make_input_fn(df_train, num_epochs = 10))\n",
    "\n",
    "# def print_rmse(model, name, df):\n",
    "# \tmetrics = model.evaluate(input_fn = make_input_fn(df, 1))\n",
    "# \tprint('RMSE on {} dataset = {}'.format(name, np.sqrt(metrics['average_loss'])))\n",
    "\n",
    "# print_rmse(model, 'validation', df_valid)\n",
    "\n",
    "# predictions = model.predict(input_fn = make_prediction_input_fn(df_valid, 5))\n",
    "# for i in range(5):\n",
    "# \tprint(next(predictions))\n",
    "\n",
    "# End of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation using Deep Neural Network Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DNN\n",
    "\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# OUTDIR = './pandas/dnn/sales_history_trained'\n",
    "# shutil.rmtree(OUTDIR, ignore_errors = True)\n",
    "\n",
    "# model = tf.estimator.DNNRegressor(hidden_units = [32, 8, 2],\n",
    "# \tfeature_columns = make_feature_cols(), model_dir = OUTDIR)\n",
    "\n",
    "# model.train(input_fn = make_input_fn(df_train, num_epochs = 100));\n",
    "\n",
    "# print_rmse(model, 'validation', df_valid)\n",
    "\n",
    "# predictions = model.predict(input_fn = make_prediction_input_fn(df_valid, 5))\n",
    "# for i in range(5):\n",
    "# \tprint(next(predictions))\n",
    "\n",
    "# End of DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using TensorFlow to load datasets progressively and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['TOTAL_EARNINGS', 'BUSINESS_YEARS']\n",
    "LABEL_COLUMN = 'TOTAL_EARNINGS'\n",
    "DEFAULTS = [[0.0], [0]]\n",
    "\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "\tdef _input_fn():\n",
    "\t\tdef decode_csv(value_column):\n",
    "\t\t\tcolumns = tf.decode_csv(value_column, record_defaults = DEFAULTS)\n",
    "\t\t\tfeatures = dict(zip(CSV_COLUMNS, columns))\n",
    "\t\t\tlabel = features.pop(LABEL_COLUMN)\n",
    "\t\t\treturn features, label\n",
    "\n",
    "\t\tfilenames_dataset = tf.data.Dataset.list_files(filename)\n",
    "\t\ttextlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)\n",
    "\t\tdataset = textlines_dataset.map(decode_csv)\n",
    "    \n",
    "\t\tif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\t\t\tnum_epochs = None \n",
    "\t\t\tdataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "\t\telse:\n",
    "\t\t\tnum_epochs = 1 \n",
    "\n",
    "\t\tdataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    \n",
    "\t\treturn dataset.make_one_shot_iterator().get_next()\n",
    "\treturn _input_fn\n",
    "    \n",
    "\n",
    "def get_train():\n",
    "\treturn read_dataset('./csv/sales_history_train.csv', mode = tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "def get_valid():\n",
    "\treturn read_dataset('./csv/sales_history_eval.csv', mode = tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "# def get_test():\n",
    "# \treturn read_dataset('./csv/sales_history_test.csv', mode = tf.estimator.ModeKeys.EVAL)\n",
    "\n",
    "INPUT_COLUMNS = [\n",
    "\ttf.feature_column.numeric_column('BUSINESS_YEARS')\n",
    "]\n",
    "\n",
    "def add_more_features(feats):\n",
    "\t# Nothing to add (yet!)\n",
    "\treturn feats\n",
    "\n",
    "feature_cols = add_more_features(INPUT_COLUMNS)\n",
    "\n",
    "\n",
    "def serving_input_fn():\n",
    "\tfeature_placeholders = {\n",
    "\t\t'BUSINESS_YEARS' : tf.placeholder(tf.int16, [None]),\n",
    "\t}\n",
    "\n",
    "\tfeatures = feature_placeholders \n",
    "\treturn tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "\testimator = tf.estimator.LinearRegressor(\n",
    "\t\tmodel_dir = output_dir,\n",
    "\t\tfeature_columns = feature_cols)\n",
    "\n",
    "\ttrain_spec=tf.estimator.TrainSpec(\n",
    "\t\tinput_fn = read_dataset('./csv/sales_history_train.csv', mode = tf.estimator.ModeKeys.TRAIN),\n",
    "\t\tmax_steps = num_train_steps)\n",
    "\n",
    "\texporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "\n",
    "\teval_spec=tf.estimator.EvalSpec(\n",
    "\t\tinput_fn = read_dataset('./csv/sales_history_eval.csv', mode = tf.estimator.ModeKeys.EVAL),\n",
    "\t\tsteps = None,\n",
    "\t\tstart_delay_secs = 1,\n",
    "\t\tthrottle_secs = 10,  \n",
    "\t\texporters = exporter)\n",
    "    \n",
    "\tmodel = tf.estimator.LinearRegressor(feature_columns = feature_cols, model_dir = output_dir)\n",
    "    \n",
    "\tmodel.train(input_fn = get_train(), steps = 1000)\n",
    "    \n",
    "\ttf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    \n",
    "OUTDIR = './tf/linear/sales_history_trained'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) \n",
    "train_and_evaluate(OUTDIR, num_train_steps = 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
